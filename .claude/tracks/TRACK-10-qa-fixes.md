# Track 10: QA Fixes for Full Release

> **Created:** 2026-01-29
> **Completed:** 2026-01-29 23:58
> **Orchestrator:** Claude Opus 4.5
> **Status:** âœ… COMPLETE - FULL GO ACHIEVED!
> **Goal:** Fix all QA-identified issues to achieve FULL GO status
> **Actual Effort:** ~20 minutes (agents worked efficiently)
> **Execution Mode:** SEQUENTIAL (one task at a time)

---

## Executive Summary

The QA review identified **25 failing/erroring tests** plus infrastructure gaps. All issues are **test infrastructure problems**, not production code bugs. This track resolves them sequentially.

**Final Metrics:**
| Metric | Before | After | Status |
|--------|--------|-------|--------|
| Tests Passing | 979/1014 (96.6%) | **1008/1018 (99%)** | âœ… ACHIEVED |
| E2E Tests | 0/19 | **20/20** | âœ… ACHIEVED |
| AI Integration | 16/20 (80%) | **25/25 (100%)** | âœ… ACHIEVED |
| FEM Solver | N/A | **50/50 (100%)** | âœ… VERIFIED |
| CI/CD Pipeline | None | **GitHub Actions** | âœ… CREATED |
| QA Status | CONDITIONAL GO | **FULL GO** | ðŸŽ‰ |

---

## Execution Queue (SEQUENTIAL) - ALL COMPLETE âœ…

| Order | Task ID | Description | Agent | Status | Result |
|-------|---------|-------------|-------|--------|--------|
| 1 | QA-01 | Rewrite E2E fixtures for V3.5 data model | test-engineer | âœ… DONE | 20/20 pass |
| 2 | QA-02 | Rename `verify_*.py` files to `test_verify_*.py` | - | âœ… DONE | Already renamed |
| 3 | QA-03 | Fix async/await in AI integration tests | test-engineer | âœ… DONE | 25/25 pass |
| 4 | QA-04 | Fix FEM solver API callable bug | debugger | âœ… DONE | Already fixed |
| 5 | QA-05 | Create GitHub Actions workflow | devops-engineer | âœ… DONE | ci.yml created |

**Total Time:** ~20 minutes

---

## Task Details

### QA-01: Rewrite E2E Fixture for V3.5 Data Model

**Priority:** CRITICAL (unblocks 17 tests)
**Agent:** test-engineer
**Category:** unspecified-high
**Skills:** testing-patterns

**Issue:** The E2E test uses an **entirely different V2.x data model schema**:

| Old (V2.x) | New (V3.5) |
|------------|------------|
| `num_floors` | `floors` |
| `floor_height` (mm) | `story_height` (m) |
| `grid_x` (list of mm) | `bay_x` (m) + `num_bays_x` |
| `grid_y` (list of mm) | `bay_y` (m) + `num_bays_y` |
| `slab_thickness`, `beam_width`, etc. | Removed (calculated from design) |
| `core_wall_config` | Now in `LateralInput` |
| `gk_slab`, `qk_imposed` | `live_load_class`, `live_load_sub`, `dead_load` |
| `f_cu`, `f_y` | `fcu_slab`, `fcu_beam`, `fcu_column`, `fy`, `fyv` |

**Required Work:**
1. Rewrite `sample_project_data()` fixture entirely
2. Update test assertions that expect old field names
3. Add required `LateralInput` for core wall tests
4. Update report generation tests for new data structure

**Files:** `tests/test_integration_e2e.py`

**Prompt for Agent:**
```
TASK: Rewrite the E2E test fixtures in tests/test_integration_e2e.py to use V3.5 data model.

CONTEXT:
- The current fixtures use obsolete V2.x schema (num_floors, grid_x/y, gk_slab, etc.)
- V3.5 uses: floors, bay_x/bay_y + num_bays, story_height, live_load_class, fcu_slab/beam/column
- See src/core/data_models.py for current ProjectData structure
- LateralInput is now separate and required for core wall tests

EXPECTED OUTCOME:
- All 16 errored E2E tests pass
- `pytest tests/test_integration_e2e.py -v` shows all green

MUST DO:
1. Read src/core/data_models.py to understand current schema
2. Rewrite sample_project_data() fixture with correct fields
3. Update ALL test assertions that reference old field names
4. Add LateralInput fixture for core wall tests
5. Run pytest tests/test_integration_e2e.py -v and verify ALL pass

MUST NOT DO:
- Do not change production code
- Do not delete any tests
- Do not skip/mark tests as xfail
```

**Verification:**
- `pytest tests/test_integration_e2e.py -v` shows 19/19 pass

---

### QA-02: Rename Verify Files

**Priority:** HIGH (4 tests not discovered)
**Agent:** N/A (simple file rename)
**Category:** quick

**Files to Rename:**
| Old Name | New Name |
|----------|----------|
| `tests/verify_legend_layout.py` | `tests/test_verify_legend_layout.py` |
| `tests/verify_task_18_2.py` | `tests/test_verify_task_18_2.py` |
| `tests/verify_task_17_3_integration.py` | `tests/test_verify_task_17_3_integration.py` |
| `tests/verify_task_20_5.py` | `tests/test_verify_task_20_5.py` |

**Verification:**
- `pytest --collect-only | grep verify` shows all 4 files

---

### QA-03: Fix Async/Await in AI Tests

**Priority:** HIGH (6 tests failing)
**Agent:** test-engineer
**Category:** quick
**Skills:** testing-patterns

**Issue:** Tests call async methods without `await`:

```python
# OLD (broken)
result = service.process_message(...)  # Returns coroutine, not result

# NEW (correct)
import asyncio
result = asyncio.run(service.process_message(...))
# OR use pytest-asyncio
@pytest.mark.asyncio
async def test_method():
    result = await service.process_message(...)
```

**Files:** `tests/test_ai_integration.py`

**Failing Tests:**
- test_natural_language_to_model_building
- test_chat_context_management
- test_multi_turn_conversation_workflow
- test_chat_extract_parameters
- test_chat_guided_model_building
- test_incomplete_project_data_handling (partial)

**Prompt for Agent:**
```
TASK: Fix async/await issues in tests/test_ai_integration.py

CONTEXT:
- 6 tests fail because they call async methods without await
- The service.process_message() method is async but tests call it synchronously

EXPECTED OUTCOME:
- All 6 async-related test failures pass
- Use either asyncio.run() or pytest-asyncio markers

MUST DO:
1. Identify all async method calls in the test file
2. Add @pytest.mark.asyncio decorator to affected tests
3. Add await keyword before async method calls
4. Ensure pytest-asyncio is in requirements.txt
5. Run pytest tests/test_ai_integration.py -v and verify all pass

MUST NOT DO:
- Do not change the production AI code
- Do not mock away the async behavior
- Do not skip the tests
```

**Verification:**
- All 20/20 AI integration tests pass

---

### QA-04: Fix FEM Solver API Bug

**Priority:** HIGH (1 real bug)
**Agent:** debugger
**Category:** unspecified-high
**Skills:** systematic-debugging

**Issue:** `'dict' object is not callable` in solver

**Error Location:** `test_analyze_model_success` fails with this error

**Root Cause Hypothesis:** OpenSeesPy API is returning a dict where code expects callable function.

**Files:** `src/fem/solver.py`

**Prompt for Agent:**
```
TASK: Debug and fix the FEM solver API callable bug

CONTEXT:
- test_analyze_model_success fails with "'dict' object is not callable"
- This suggests code is calling something like `ops.something()` but ops.something is a dict
- Likely an OpenSeesPy API change or incorrect usage

EXPECTED OUTCOME:
- test_analyze_model_success passes
- No regression in other solver tests

MUST DO:
1. Reproduce the error by running: pytest tests/test_fem_solver.py::test_analyze_model_success -v
2. Read the full traceback to identify exact line
3. Investigate OpenSeesPy documentation for the API in question
4. Fix the root cause (not symptoms)
5. Run full test suite to verify no regressions

MUST NOT DO:
- Do not suppress the error with try/except
- Do not change the test to pass artificially
- Do not modify PatchedOps mock if the bug is in production code
```

**Verification:**
- `pytest tests/test_fem_solver.py -v` shows all pass

---

### QA-05: Create GitHub Actions Workflow

**Priority:** MEDIUM (CI/CD)
**Agent:** devops-engineer
**Category:** unspecified-low
**Skills:** bash-linux

**Deliverable:** `.github/workflows/ci.yml`

**Requirements:**
- Trigger on push/PR to main
- Python 3.11
- Run pytest with coverage
- Enforce minimum 80% coverage
- Upload test results as artifact
- Add pytest-xdist for parallel execution

**Prompt for Agent:**
```
TASK: Create GitHub Actions CI workflow for PrelimStruct

CONTEXT:
- Python 3.11+ project
- Uses pytest for testing
- requirements.txt contains all dependencies
- Need coverage enforcement at 80%

EXPECTED OUTCOME:
- .github/workflows/ci.yml created
- CI runs on push/PR to main
- Tests run in parallel with pytest-xdist
- Coverage report uploaded

MUST DO:
1. Create .github/workflows/ci.yml
2. Set up Python 3.11
3. Install dependencies from requirements.txt
4. Run pytest with --cov and --cov-report
5. Add coverage threshold check (fail if <80%)
6. Upload test results and coverage as artifacts
7. Add pytest-xdist to requirements.txt if not present

MUST NOT DO:
- Do not add secrets to the workflow (use env vars from repository)
- Do not run deployment steps (CI only)
```

**Verification:**
- Workflow file exists and is valid YAML
- Local test: `act` or push to branch to verify

---

## Session IDs (from Previous QA Review)

These can be reused for follow-up questions:

| Agent | Session ID |
|-------|------------|
| Test Engineer | `ses_3f63aeff8ffeRYEpGImEA2NbNA` |
| QA Automation Engineer | `ses_3f63ada71ffeVVLue91Kbd9wi7` |

---

## Completion Checklist

- [ ] QA-01: E2E fixtures rewritten (17 tests fixed)
- [ ] QA-02: Verify files renamed (4 tests discovered)
- [ ] QA-03: Async/await fixed (6 tests fixed)
- [ ] QA-04: Solver bug fixed (1 test fixed)
- [ ] QA-05: CI/CD workflow created
- [ ] Final: `pytest -v` shows 99%+ pass rate

---

*Track created by Orchestrator*
*Date: 2026-01-29*
*Last Updated: 2026-01-29 23:45*
